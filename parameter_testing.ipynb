{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Read out 100 rows rather than the default value. \n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Not prinitng the warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting the working path for data input and result outputs\n",
    "os.chdir('D:\\\\Spring 2019\\\\DS 440\\\\Data')\n",
    "\n",
    "# Setting a random seed for reproducability\n",
    "np.random.seed(7)\n",
    "\n",
    "# Setting up the k-fold\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 7)\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER = /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "df = pd.read_csv('kplr_dr25_inj1_plti.csv', header = 0)\n",
    "\n",
    "print('Dataset Size:')\n",
    "print(df.shape)\n",
    "print()\n",
    "\n",
    "temp_df = df.iloc[:, 0:15]\n",
    "df_drop = temp_df[temp_df.isnull().any(axis=1)]\n",
    "temp_df = temp_df.drop(df_drop.index.values)\n",
    "temp_df = temp_df[temp_df.Recovered != 2]\n",
    "\n",
    "print('Cleaned Dataset Size:')\n",
    "print(temp_df.shape)\n",
    "print()\n",
    "\n",
    "#X = temp_df.iloc[:, 1:14]\n",
    "#Y = temp_df.iloc[:, 14]\n",
    "\n",
    "X_cols = ['i_period', 'i_epoch', 'N_Transit', 'i_depth', 'i_dur', 'i_ror', 'i_dor', 'Expected_MES']\n",
    "Y_cols = ['Recovered']\n",
    "\n",
    "X = temp_df[X_cols]\n",
    "Y = temp_df[Y_cols]\n",
    "\n",
    "print('Input Size:', X.shape)\n",
    "print('Output Size:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Problem 2\n",
    "df = pd.read_csv('kplr_dr25_inj1_tces.csv', header = 0)\n",
    "\n",
    "print('Dataset Size: ')\n",
    "print(df.shape)\n",
    "print()\n",
    "\n",
    "cols = ['TCE_ID', 'KIC', 'Disp', 'Score', 'period', 'epoch', 'NTL', 'SS', \n",
    "        'CO', 'EM', 'Expected_MES', 'MES', 'NTran', 'depth', 'duration', 'Rp',\n",
    "        'Rs', 'Ts', 'logg', 'a', 'Rp/Rs', 'a/Rs', 'impact', 'SNR_DV', 'Sp',\n",
    "        'Fit_Prov']\n",
    "df = df[cols]\n",
    "df.columns\n",
    "\n",
    "df['Disp'] = df['Disp'].replace('PC', 1)\n",
    "df['Disp'] = df['Disp'].replace('FP', 0)\n",
    "\n",
    "X = df.iloc[:, 6:25]\n",
    "Y = df.iloc[:, 2]\n",
    "\n",
    "print('Input Size:', X.shape)\n",
    "print('Output Size:', Y.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(n_estimators = [int(x) for x in np.linspace(start = 10, stop = 350, num = 10)],\n",
    "                  learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                  algorithm = ['SAMME', 'SAMME.R'])\n",
    "\n",
    "print('AdaBoost Classifier')\n",
    "abc_grid_search = GridSearchCV(abc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "abc_grid_result = abc_grid_search.fit(X, Y)\n",
    "abc_predict = abc_grid_search.predict(X)\n",
    "abc_predict_proba = pd.DataFrame(abc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "abc_accuracy = metrics.accuracy_score(Y, abc_predict)  \n",
    "abc_precision = metrics.precision_score(Y, abc_predict, pos_label=1)\n",
    "abc_recall = metrics.recall_score(Y, abc_predict, pos_label=1)  \n",
    "abc_f1 = metrics.f1_score(Y, abc_predict, pos_label=1)\n",
    "abc_auroc = metrics.roc_auc_score(Y, abc_predict)\n",
    "abc_aurpc = metrics.average_precision_score(Y, abc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('AdaBoostClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "dtc = DecisionTreeClassifier()\n",
    "    \n",
    "# Updating the param grid\n",
    "param_grid = dict(criterion = ['gini', 'entropy'],\n",
    "                  max_depth = [int(x) for x in np.linspace(start = 2, stop = 30, num = 10)],\n",
    "                  max_features = ['sqrt', 'log2', None],\n",
    "                  min_impurity_decrease = [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "                  min_samples_split = [2, 4, 6, 8, 10],\n",
    "                  min_samples_leaf = [0.10, 0.25, 0.50, 1, 2, 4])\n",
    "\n",
    "print('DecisionTree Classifier')\n",
    "dtc_grid_search = GridSearchCV(dtc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "dtc_grid_result = dtc_grid_search.fit(X, Y)\n",
    "dtc_predict = dtc_grid_search.predict(X)\n",
    "dtc_predict_proba = pd.DataFrame(dtc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "dtc_accuracy = metrics.accuracy_score(Y, dtc_predict)  \n",
    "dtc_precision = metrics.precision_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_recall = metrics.recall_score(Y, dtc_predict, pos_label=1)  \n",
    "dtc_f1 = metrics.f1_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_auroc = metrics.roc_auc_score(Y, dtc_predict_proba[1])\n",
    "dtc_aurpc = metrics.average_precision_score(Y, dtc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('DecisionTreeClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "etc = ExtraTreesClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(max_depth = [int(x) for x in np.linspace(start = 2, stop = 30, num = 10)],\n",
    "                  max_features = ['sqrt', 'log2', None],\n",
    "                  min_impurity_decrease = [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "                  min_samples_split = [2, 4, 6, 8, 10],\n",
    "                  min_samples_leaf = [0.10, 0.25, 0.50, 1, 2, 4],\n",
    "                  n_estimators = [int(x) for x in np.linspace(start = 10, stop = 350, num = 10)])\n",
    "\n",
    "print('ExtraTrees Classifier')\n",
    "etc_grid_search = GridSearchCV(etc, param_grid, scoring = 'accuracy', n_jobs = 4, cv = kfold, verbose = 1)\n",
    "etc_grid_result = etc_grid_search.fit(X, Y)\n",
    "etc_predict = etc_grid_search.predict(X)\n",
    "etc_predict_proba = pd.DataFrame(etc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "etc_accuracy = metrics.accuracy_score(Y, etc_predict)  \n",
    "etc_precision = metrics.precision_score(Y, etc_predict, pos_label=1)\n",
    "etc_recall = metrics.recall_score(Y, etc_predict, pos_label=1)  \n",
    "etc_f1 = metrics.f1_score(Y, etc_predict, pos_label=1)\n",
    "etc_auroc = metrics.roc_auc_score(Y, etc_predict_proba[1])\n",
    "etc_aurpc = metrics.average_precision_score(Y, etc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('ExtraTreesClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict()\n",
    "        \n",
    "print('Naive Bayes')\n",
    "gnb_grid_search = GridSearchCV(gnb, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "gnb_grid_result = gnb_grid_search.fit(X, Y)\n",
    "gnb_predict = gnb_grid_search.predict(X)\n",
    "gnb_predict_proba = pd.DataFrame(gnb_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "gnb_accuracy = metrics.accuracy_score(Y, gnb_predict)  \n",
    "gnb_precision = metrics.precision_score(Y, gnb_predict, pos_label=1)\n",
    "gnb_recall = metrics.recall_score(Y, gnb_predict, pos_label=1)  \n",
    "gnb_f1 = metrics.f1_score(Y, gnb_predict, pos_label=1)\n",
    "gnb_auroc = metrics.roc_auc_score(Y, gnb_predict_proba[1])\n",
    "gnb_aurpc = metrics.average_precision_score(Y, gnb_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('GaussianNB_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "gpc = GaussianProcessClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(max_iter_predict = [50, 100, 150, 200])\n",
    "        \n",
    "print('GaussianProcess Classifier')\n",
    "gpc_grid_search = GridSearchCV(gpc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "gpc_grid_result = gpc_grid_search.fit(X, Y)\n",
    "gpc_predict = gpc_grid_search.predict(X)\n",
    "gpc_predict_proba = pd.DataFrame(gpc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "gpc_accuracy = metrics.accuracy_score(Y, gpc_predict)  \n",
    "gpc_precision = metrics.precision_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_recall = metrics.recall_score(Y, gpc_predict, pos_label=1)  \n",
    "gpc_f1 = metrics.f1_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_auroc = metrics.roc_auc_score(Y, gpc_predict_proba[1])\n",
    "gpc_aurpc = metrics.average_precision_score(Y, gpc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('GaussianProcessClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "gbc = GradientBoostingClassifier()\n",
    "        \n",
    "# Updating the param grid\n",
    "param_grid = dict(criterion = ['friedman_mse', 'mse', 'mae'],\n",
    "                  learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                  loss = ['deviance', 'exponential'],\n",
    "                  max_depth = [int(x) for x in np.linspace(start = 2, stop = 30, num = 10)],\n",
    "                  max_features = ['sqrt', 'log2', None],\n",
    "                  min_impurity_decrease = [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "                  min_samples_split = [2, 4, 6, 8, 10],\n",
    "                  min_samples_leaf = [0.10, 0.25, 0.50, 1, 2, 4],\n",
    "                  n_estimators = [int(x) for x in np.linspace(start = 10, stop = 350, num = 10)],\n",
    "                  subsample = [0.5, 1.0])\n",
    "        \n",
    "print('GradientBoosting Classifier')\n",
    "gbc_grid_search = GridSearchCV(gbc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "gbc_grid_result = gbc_grid_search.fit(X, Y)\n",
    "gbc_predict = gbc_grid_search.predict(X)\n",
    "gbc_predict_proba = pd.DataFrame(gbc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "gbc_accuracy = metrics.accuracy_score(Y, gbc_predict)  \n",
    "gbc_precision = metrics.precision_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_recall = metrics.recall_score(Y, gbc_predict, pos_label=1)  \n",
    "gbc_f1 = metrics.f1_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_auroc = metrics.roc_auc_score(Y, gbc_predict_proba[1])\n",
    "gbc_aurpc = metrics.average_precision_score(Y, gbc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('GradientBoostingClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                  leaf_size = [10, 20, 30, 40, 50],\n",
    "                  metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "                  n_neighbors = [int(x) for x in np.linspace(start = 3, stop = 30, num = 10)],\n",
    "                  p = [float(x) for x in np.linspace(start = 1, stop = 5, num = 10)],\n",
    "                  weights = ['uniform', 'distance'])\n",
    "\n",
    "print('KNeighbors Classifier')\n",
    "knc_grid_search = GridSearchCV(knc, param_grid, scoring = 'accuracy', n_jobs = 4, cv = kfold, verbose = 1)\n",
    "knc_grid_result = knc_grid_search.fit(X, Y)\n",
    "knc_predict = knc_grid_search.predict(X)\n",
    "knc_predict_proba = pd.DataFrame(knc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "knc_accuracy = metrics.accuracy_score(Y, knc_predict)  \n",
    "knc_precision = metrics.precision_score(Y, knc_predict, pos_label=1)\n",
    "knc_recall = metrics.recall_score(Y, knc_predict, pos_label=1)  \n",
    "knc_f1 = metrics.f1_score(Y, knc_predict, pos_label=1)\n",
    "knc_auroc = metrics.roc_auc_score(Y, knc_predict_proba[1])\n",
    "knc_aurpc = metrics.average_precision_score(Y, knc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('KNeighborsClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                  alpha = [0.0005, 0.0001, 0.005, 0.001, 0.01, 0.1],\n",
    "                  batch_size = [32, 64, 96, 128, 256],\n",
    "                  hidden_layer_sizes = [(50, 100, 50), (100, 50, 100), (50, 100), (100, 50), (100, )],\n",
    "                  learning_rate = ['constant', 'invscaling', 'adaptive'],\n",
    "                  learning_rate_init = [0.0001, 0.001, 0.01, 0.1],\n",
    "                  max_iter = [250, 500, 1000],\n",
    "                  solver = ['lbfgs', 'sgd', 'adam'])\n",
    "\n",
    "print('Multi-layer Perceptron Classifier')\n",
    "mlp_grid_search = GridSearchCV(mlp, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "mlp_grid_result = mlp_grid_search.fit(X, Y)\n",
    "mlp_predict = mlp_grid_search.predict(X)\n",
    "mlp_predict_proba = pd.DataFrame(mlp_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "mlp_accuracy = metrics.accuracy_score(Y, mlp_predict)  \n",
    "mlp_precision = metrics.precision_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_recall = metrics.recall_score(Y, mlp_predict, pos_label=1)  \n",
    "mlp_f1 = metrics.f1_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_auroc = metrics.roc_auc_score(Y, mlp_predict_proba[1])\n",
    "mlp_aurpc = metrics.average_precision_score(Y, mlp_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('MLPClassifier_Parameter_Tuning.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(max_depth = [int(x) for x in np.linspace(start = 2, stop = 30, num = 10)],\n",
    "                  max_features = ['sqrt', 'log2', None],\n",
    "                  min_impurity_decrease = [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "                  min_samples_split = [2, 4, 6, 8, 10],\n",
    "                  min_samples_leaf = [0.10, 0.25, 0.50, 1, 2, 4],\n",
    "                  n_estimators = [int(x) for x in np.linspace(start = 10, stop = 350, num = 5)])\n",
    "\n",
    "print('RandomForest Classifier')\n",
    "rfc_grid_search = GridSearchCV(rfc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "rfc_grid_result = rfc_grid_search.fit(X, Y)\n",
    "rfc_predict = rfc_grid_search.predict(X)\n",
    "rfc_predict_proba = pd.DataFrame(rfc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "rfc_accuracy = metrics.accuracy_score(Y, rfc_predict)  \n",
    "rfc_precision = metrics.precision_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_recall = metrics.recall_score(Y, rfc_predict, pos_label=1)  \n",
    "rfc_f1 = metrics.f1_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_auroc = metrics.roc_auc_score(Y, rfc_predict_proba[1])\n",
    "rfc_aurpc = metrics.average_precision_score(Y, rfc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('RandomForestClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Instantiate the model\n",
    "svc = SVC()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(C = [0.001, 0.01, 0.1, 1, 10],\n",
    "                  decision_function_shape = ['ovo', 'ovr'],\n",
    "                  degree = [0, 1, 2, 3, 4, 5, 6],\n",
    "                  gamma = [0.001, 0.01, 0.1, 1, 10],\n",
    "                  kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                  shrinking = ['True', 'False'])\n",
    "        \n",
    "print('ExtraTrees Classifier')\n",
    "svc_grid_search = GridSearchCV(svc, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "svc_grid_result = svc_grid_search.fit(X, Y)\n",
    "svc_predict = svc_grid_search.predict(X)\n",
    "svc_predict_proba = pd.DataFrame(svc_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "svc_accuracy = metrics.accuracy_score(Y, svc_predict)  \n",
    "svc_precision = metrics.precision_score(Y, svc_predict, pos_label=1)\n",
    "svc_recall = metrics.recall_score(Y, svc_predict, pos_label=1)  \n",
    "svc_f1 = metrics.f1_score(Y, svc_predict, pos_label=1)\n",
    "svc_auroc = metrics.roc_auc_score(Y, svc_predict_proba[1])\n",
    "svc_aurpc = metrics.average_precision_score(Y, svc_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('SVC_Parameter_Tuning.db')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Updating the param grid\n",
    "param_grid = dict(learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                  min_split_loss = [0.0, 0.2, 0.4,],\n",
    "                  max_depth = [int(x) for x in np.linspace(start = 2, stop = 30, num = 5)],\n",
    "                  min_child_weight = [1, 3, 5],\n",
    "                  reg_lambda = [0.0001, 0.001, 0.01, 0.1],\n",
    "                  reg_alpha = [0.0001, 0.001, 0.01, 0.1])\n",
    "\n",
    "print('XGBoost Classifier')\n",
    "xgb_grid_search = GridSearchCV(xgb, param_grid, scoring = 'accuracy', n_jobs = -1, cv = kfold, verbose = 1)\n",
    "xgb_grid_result = xgb_grid_search.fit(X, Y)\n",
    "xgb_predict = xgb_grid_search.predict(X)\n",
    "xgb_predict_proba = pd.DataFrame(xgb_grid_search.predict_proba(X))\n",
    "\n",
    "# Store metrics\n",
    "xgb_accuracy = metrics.accuracy_score(Y, xgb_predict)  \n",
    "xgb_precision = metrics.precision_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_recall = metrics.recall_score(Y, xgb_predict, pos_label=1)  \n",
    "xgb_f1 = metrics.f1_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_auroc = metrics.roc_auc_score(Y, xgb_predict_proba[1])\n",
    "xgb_aurpc = metrics.average_precision_score(Y, xgb_predict, pos_label=1)\n",
    "\n",
    "dill.dump_session('XGBClassifier_Parameter_Tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['ADA Boost', 'Extra Trees', 'Random Forest', 'Gradient Boosting', 'XG Boost', 'Decision Tree',\n",
    "              'Multi Layer Perceptron', 'K Neighbors', 'Naive Bayes', 'Aggregate'],\n",
    "    'Accuracy' : [abc_accuracy, etc_accuracy, rfc_accuracy, gbc_accuracy, xgb_accuracy, dtc_accuracy,\n",
    "                  mlp_accuracy, knc_accuracy, gnb_accuracy, aggregate_accuracy],\n",
    "    'F1' : [abc_f1, etc_f1, rfc_f1, gbc_f1, xgb_f1, dtc_f1, mlp_f1, knc_f1, gnb_f1, aggregate_f1],\n",
    "    'AUROC' : [abc_auroc, etc_auroc, rfc_auroc, gbc_auroc, xgb_auroc, dtc_auroc, mlp_auroc, knc_auroc, \n",
    "               gnb_auroc, aggregate_auroc],\n",
    "    'AURPC' : [abc_aurpc, etc_aurpc, rfc_aurpc, gbc_aurpc, xgb_aurpc, dtc_aurpc, mlp_aurpc, knc_aurpc,\n",
    "               gnb_aurpc, aggregate_aurpc],\n",
    "    'Precision': [abc_precision, etc_precision, rfc_precision, gbc_precision, xgb_precision, dtc_precision, mlp_precision, \n",
    "                  knc_precision, gnb_precision, aggregate_precision],\n",
    "    'Recall' : [abc_recall, etc_recall, rfc_recall, gbc_recall, xgb_recall, dtc_recall, mlp_recall, knc_recall,\n",
    "                gnb_recall, aggregate_recall]\n",
    "})\n",
    "# Print table and sort by test precision\n",
    "models = models.sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "blankIndex=[''] * len(models)\n",
    "models.index=blankIndex\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "means = _grid_result.cv_results_['mean_test_score']\n",
    "stds = _grid_result.cv_results_['std_test_score']\n",
    "params = _grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
