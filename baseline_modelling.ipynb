{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import warnings\n",
    "\n",
    "import time as time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Read out 100 rows rather than the default value. \n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Not prinitng the warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting the working path for data input and result outputs\n",
    "os.chdir('D:\\\\Spring 2019\\\\DS 440\\\\Data')\n",
    "\n",
    "# Setting a random seed for reproducability\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: \n",
      "(146294, 25)\n"
     ]
    }
   ],
   "source": [
    "#filenames = glob('D:\\Spring 2019\\DS 440\\Data\\kplr_dr25_inj*.csv')\n",
    "#df = pd.concat([pd.read_csv(f) for f in filenames], ignore_index = True)\n",
    "\n",
    "# Problem 1\n",
    "df = pd.read_csv('kplr_dr25_inj1_plti.csv', header = 0)\n",
    "\n",
    "# Problem 2\n",
    "#df = pd.read_csv('kplr_dr25_inj1_tces.csv', header = 0)\n",
    "\n",
    "print('Size: ')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncols = ['TCE_ID', 'KIC', 'Disp', 'Score', 'period', 'epoch', 'NTL', 'SS', \\n        'CO', 'EM', 'Expected_MES', 'MES', 'NTran', 'depth', 'duration', 'Rp',\\n        'Rs', 'Ts', 'logg', 'a', 'Rp/Rs', 'a/Rs', 'impact', 'SNR_DV', 'Sp',\\n        'Fit_Prov']\\ndf = df[cols]\\ndf.columns\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cols = ['TCE_ID', 'KIC', 'Disp', 'Score', 'period', 'epoch', 'NTL', 'SS', \n",
    "        'CO', 'EM', 'Expected_MES', 'MES', 'NTran', 'depth', 'duration', 'Rp',\n",
    "        'Rs', 'Ts', 'logg', 'a', 'Rp/Rs', 'a/Rs', 'impact', 'SNR_DV', 'Sp',\n",
    "        'Fit_Prov']\n",
    "df = df[cols]\n",
    "df.columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145671, 15)\n",
      "dict_keys([0, 1])\n",
      "dict_values([100880, 44791])\n"
     ]
    }
   ],
   "source": [
    "temp_df = df.iloc[:, 0:15]\n",
    "df_drop = temp_df[temp_df.isnull().any(axis=1)]\n",
    "temp_df = temp_df.drop(df_drop.index.values)\n",
    "temp_df = temp_df[temp_df.Recovered != 2]\n",
    "print(temp_df.shape)\n",
    "\n",
    "X = temp_df.iloc[:, 1:14]\n",
    "Y = temp_df.iloc[:, 14]\n",
    "\n",
    "print(Counter(temp_df.Recovered).keys())\n",
    "print(Counter(temp_df.Recovered).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf['Disp'] = df['Disp'].replace('PC', 1)\\ndf['Disp'] = df['Disp'].replace('FP', 0)\\n\\nX = df.iloc[:, 10:25]\\nY = df.iloc[:, 2]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df['Disp'] = df['Disp'].replace('PC', 1)\n",
    "df['Disp'] = df['Disp'].replace('FP', 0)\n",
    "\n",
    "X = df.iloc[:, 10:25]\n",
    "Y = df.iloc[:, 2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "svc = SVC()\n",
    "dtc = DecisionTreeClassifier()\n",
    "mlp = MLPClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "gpc = GaussianProcessClassifier()\n",
    "\n",
    "kFold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  149.2\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "abc_predict = cross_val_predict(abc, X, Y, cv = kFold)\n",
    "abc_predict_proba = pd.DataFrame(cross_val_predict(abc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "abc_accuracy = metrics.accuracy_score(Y, abc_predict)  \n",
    "abc_precision = metrics.precision_score(Y, abc_predict, pos_label=1)\n",
    "abc_recall = metrics.recall_score(Y, abc_predict, pos_label=1)  \n",
    "abc_f1 = metrics.f1_score(Y, abc_predict, pos_label=1)\n",
    "abc_auroc = metrics.roc_auc_score(Y, abc_predict_proba[1])\n",
    "abc_aurpc = metrics.average_precision_score(Y, abc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  20.32\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "etc_predict = cross_val_predict(etc, X, Y, cv = kFold)\n",
    "etc_predict_proba = pd.DataFrame(cross_val_predict(etc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "etc_accuracy = metrics.accuracy_score(Y, etc_predict)  \n",
    "etc_precision = metrics.precision_score(Y, etc_predict, pos_label=1)\n",
    "etc_recall = metrics.recall_score(Y, etc_predict, pos_label=1)  \n",
    "etc_f1 = metrics.f1_score(Y, etc_predict, pos_label=1)\n",
    "etc_auroc = metrics.roc_auc_score(Y, etc_predict_proba[1])\n",
    "etc_aurpc = metrics.average_precision_score(Y, etc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  50.96\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rfc_predict = cross_val_predict(rfc, X, Y, cv = kFold)\n",
    "rfc_predict_proba = pd.DataFrame(cross_val_predict(rfc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "rfc_accuracy = metrics.accuracy_score(Y, rfc_predict)  \n",
    "rfc_precision = metrics.precision_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_recall = metrics.recall_score(Y, rfc_predict, pos_label=1)  \n",
    "rfc_f1 = metrics.f1_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_auroc = metrics.roc_auc_score(Y, rfc_predict_proba[1])\n",
    "rfc_aurpc = metrics.average_precision_score(Y, rfc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  232.34\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gbc_predict = cross_val_predict(gbc, X, Y, cv = kFold)\n",
    "gbc_predict_proba = pd.DataFrame(cross_val_predict(gbc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "gbc_accuracy = metrics.accuracy_score(Y, gbc_predict)  \n",
    "gbc_precision = metrics.precision_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_recall = metrics.recall_score(Y, gbc_predict, pos_label=1)  \n",
    "gbc_f1 = metrics.f1_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_auroc = metrics.roc_auc_score(Y, gbc_predict_proba[1])\n",
    "gbc_aurpc = metrics.average_precision_score(Y, gbc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  232.38\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "xgb_predict = cross_val_predict(xgb, X, Y, cv = kFold)\n",
    "xgb_predict_proba = pd.DataFrame(cross_val_predict(xgb, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "xgb_accuracy = metrics.accuracy_score(Y, xgb_predict)  \n",
    "xgb_precision = metrics.precision_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_recall = metrics.recall_score(Y, xgb_predict, pos_label=1)  \n",
    "xgb_f1 = metrics.f1_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_auroc = metrics.roc_auc_score(Y, xgb_predict_proba[1])\n",
    "xgb_aurpc = metrics.average_precision_score(Y, xgb_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstart = time.time()\\nsvc_predict = cross_val_predict(svc, X, Y, cv = kFold)\\nsvc_predict_proba = pd.DataFrame(cross_val_predict(svc, X, Y, cv = kFold, method='predict_proba'))\\nend = time.time()\\n\\n# Store metrics\\nsvc_accuracy = metrics.accuracy_score(Y, svc_predict)  \\nsvc_precision = metrics.precision_score(Y, svc_predict, pos_label=1)\\nsvc_recall = metrics.recall_score(Y, svc_predict, pos_label=1)  \\nsvc_f1 = metrics.f1_score(Y, svc_predict, pos_label=1)\\nsvc_auroc = metrics.roc_auc_score(Y, svc_predict_proba[1])\\nsvc_aurpc = metrics.average_precision_score(Y, svc_predict, pos_label=1)\\n\\nprint('Time: ', round(end - start, 2))\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "svc_predict = cross_val_predict(svc, X, Y, cv = kFold)\n",
    "svc_predict_proba = pd.DataFrame(cross_val_predict(svc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "svc_accuracy = metrics.accuracy_score(Y, svc_predict)  \n",
    "svc_precision = metrics.precision_score(Y, svc_predict, pos_label=1)\n",
    "svc_recall = metrics.recall_score(Y, svc_predict, pos_label=1)  \n",
    "svc_f1 = metrics.f1_score(Y, svc_predict, pos_label=1)\n",
    "svc_auroc = metrics.roc_auc_score(Y, svc_predict_proba[1])\n",
    "svc_aurpc = metrics.average_precision_score(Y, svc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  32.35\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dtc_predict = cross_val_predict(dtc, X, Y, cv = kFold)\n",
    "dtc_predict_proba = pd.DataFrame(cross_val_predict(dtc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "dtc_accuracy = metrics.accuracy_score(Y, dtc_predict)  \n",
    "dtc_precision = metrics.precision_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_recall = metrics.recall_score(Y, dtc_predict, pos_label=1)  \n",
    "dtc_f1 = metrics.f1_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_auroc = metrics.roc_auc_score(Y, dtc_predict_proba[1])\n",
    "dtc_aurpc = metrics.average_precision_score(Y, dtc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  865.63\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mlp_predict = cross_val_predict(mlp, X, Y, cv = kFold)\n",
    "mlp_predict_proba = pd.DataFrame(cross_val_predict(mlp, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "mlp_accuracy = metrics.accuracy_score(Y, mlp_predict)  \n",
    "mlp_precision = metrics.precision_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_recall = metrics.recall_score(Y, mlp_predict, pos_label=1)  \n",
    "mlp_f1 = metrics.f1_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_auroc = metrics.roc_auc_score(Y, mlp_predict_proba[1])\n",
    "mlp_aurpc = metrics.average_precision_score(Y, mlp_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  26.35\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "knc_predict = cross_val_predict(knc, X, Y, cv = kFold)\n",
    "knc_predict_proba = pd.DataFrame(cross_val_predict(knc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "knc_accuracy = metrics.accuracy_score(Y, knc_predict)  \n",
    "knc_precision = metrics.precision_score(Y, knc_predict, pos_label=1)\n",
    "knc_recall = metrics.recall_score(Y, knc_predict, pos_label=1)  \n",
    "knc_f1 = metrics.f1_score(Y, knc_predict, pos_label=1)\n",
    "knc_auroc = metrics.roc_auc_score(Y, knc_predict_proba[1])\n",
    "knc_aurpc = metrics.average_precision_score(Y, knc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstart = time.time()\\ngpc_predict = cross_val_predict(gpc, X, Y, cv = kFold)\\ngpc_predict_proba = pd.DataFrame(cross_val_predict(gpc, X, Y, cv = kFold, method='predict_proba'))\\nend = time.time()\\n\\n# Store metrics\\ngpc_accuracy = metrics.accuracy_score(Y, gpc_predict)  \\ngpc_precision = metrics.precision_score(Y, gpc_predict, pos_label=1)\\ngpc_recall = metrics.recall_score(Y, gpc_predict, pos_label=1)  \\ngpc_f1 = metrics.f1_score(Y, gpc_predict, pos_label=1)\\ngpc_auroc = metrics.roc_auc_score(Y, gpc_predict_proba[1])\\ngpc_aurpc = metrics.average_precision_score(Y, gpc_predict, pos_label=1)\\n\\nprint('Time: ', round(end - start, 2))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "gpc_predict = cross_val_predict(gpc, X, Y, cv = kFold)\n",
    "gpc_predict_proba = pd.DataFrame(cross_val_predict(gpc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "gpc_accuracy = metrics.accuracy_score(Y, gpc_predict)  \n",
    "gpc_precision = metrics.precision_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_recall = metrics.recall_score(Y, gpc_predict, pos_label=1)  \n",
    "gpc_f1 = metrics.f1_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_auroc = metrics.roc_auc_score(Y, gpc_predict_proba[1])\n",
    "gpc_aurpc = metrics.average_precision_score(Y, gpc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'ADABoost': abc_predict,\n",
    "    'ExtraTrees': etc_predict,\n",
    "    'RandomForest': rfc_predict,\n",
    "    'GradientBoosting': gbc_predict,\n",
    "    'XGBoost': xgb_predict,\n",
    "    'DecisionTree': dtc_predict,\n",
    "    'MultiLayerPerceptron': mlp_predict,\n",
    "    'KNeighbors': knc_predict\n",
    "} \n",
    "\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metrics\n",
    "predictions['Aggregate'] = predictions.mean(axis=1)\n",
    "aggregate_auroc = metrics.roc_auc_score(Y, predictions['Aggregate'])\n",
    "\n",
    "predictions['Aggregate'] = round(predictions['Aggregate']).astype(int)\n",
    "aggregate_accuracy = metrics.accuracy_score(Y, predictions['Aggregate'])  \n",
    "aggregate_precision = metrics.precision_score(Y, predictions['Aggregate'], pos_label=1)\n",
    "aggregate_recall = metrics.recall_score(Y, predictions['Aggregate'], pos_label=1)  \n",
    "aggregate_f1 = metrics.f1_score(Y, predictions['Aggregate'], pos_label=1)\n",
    "aggregate_aurpc = metrics.average_precision_score(Y, predictions['Aggregate'], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AURPC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.889450</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.956928</td>\n",
       "      <td>0.727988</td>\n",
       "      <td>0.815915</td>\n",
       "      <td>0.827063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.889312</td>\n",
       "      <td>0.821752</td>\n",
       "      <td>0.956764</td>\n",
       "      <td>0.727676</td>\n",
       "      <td>0.813870</td>\n",
       "      <td>0.829787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Aggregate</td>\n",
       "      <td>0.888186</td>\n",
       "      <td>0.813958</td>\n",
       "      <td>0.923462</td>\n",
       "      <td>0.725766</td>\n",
       "      <td>0.833298</td>\n",
       "      <td>0.795495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ADABoost</td>\n",
       "      <td>0.887013</td>\n",
       "      <td>0.819431</td>\n",
       "      <td>0.953368</td>\n",
       "      <td>0.722775</td>\n",
       "      <td>0.805565</td>\n",
       "      <td>0.833784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.880004</td>\n",
       "      <td>0.798288</td>\n",
       "      <td>0.942138</td>\n",
       "      <td>0.708024</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.772231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.868663</td>\n",
       "      <td>0.771329</td>\n",
       "      <td>0.932345</td>\n",
       "      <td>0.683912</td>\n",
       "      <td>0.830019</td>\n",
       "      <td>0.720390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>MultiLayerPerceptron</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.775038</td>\n",
       "      <td>0.939133</td>\n",
       "      <td>0.666895</td>\n",
       "      <td>0.765346</td>\n",
       "      <td>0.784979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.846366</td>\n",
       "      <td>0.750602</td>\n",
       "      <td>0.819775</td>\n",
       "      <td>0.639693</td>\n",
       "      <td>0.749316</td>\n",
       "      <td>0.751892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.717404</td>\n",
       "      <td>0.487399</td>\n",
       "      <td>0.719908</td>\n",
       "      <td>0.413898</td>\n",
       "      <td>0.551032</td>\n",
       "      <td>0.436940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy        F1     AUROC     AURPC  Precision  \\\n",
       "      GradientBoosting  0.889450  0.821452  0.956928  0.727988   0.815915   \n",
       "               XGBoost  0.889312  0.821752  0.956764  0.727676   0.813870   \n",
       "             Aggregate  0.888186  0.813958  0.923462  0.725766   0.833298   \n",
       "              ADABoost  0.887013  0.819431  0.953368  0.722775   0.805565   \n",
       "          RandomForest  0.880004  0.798288  0.942138  0.708024   0.826164   \n",
       "            ExtraTrees  0.868663  0.771329  0.932345  0.683912   0.830019   \n",
       "  MultiLayerPerceptron  0.859883  0.775038  0.939133  0.666895   0.765346   \n",
       "          DecisionTree  0.846366  0.750602  0.819775  0.639693   0.749316   \n",
       "            KNeighbors  0.717404  0.487399  0.719908  0.413898   0.551032   \n",
       "\n",
       "    Recall  \n",
       "  0.827063  \n",
       "  0.829787  \n",
       "  0.795495  \n",
       "  0.833784  \n",
       "  0.772231  \n",
       "  0.720390  \n",
       "  0.784979  \n",
       "  0.751892  \n",
       "  0.436940  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model comparison\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['ADABoost', 'ExtraTrees', 'RandomForest', 'GradientBoosting', 'XGBoost', 'DecisionTree',\n",
    "              'MultiLayerPerceptron', 'KNeighbors', 'Aggregate'],\n",
    "    'Accuracy' : [abc_accuracy, etc_accuracy, rfc_accuracy, gbc_accuracy, xgb_accuracy, dtc_accuracy,\n",
    "                  mlp_accuracy, knc_accuracy, aggregate_accuracy],\n",
    "    'F1' : [abc_f1, etc_f1, rfc_f1, gbc_f1, xgb_f1, dtc_f1, mlp_f1, knc_f1, aggregate_f1],\n",
    "    'AUROC' : [abc_auroc, etc_auroc, rfc_auroc, gbc_auroc, xgb_auroc, dtc_auroc, mlp_auroc, knc_auroc, aggregate_auroc],\n",
    "    'AURPC' : [abc_aurpc, etc_aurpc, rfc_aurpc, gbc_aurpc, xgb_aurpc, dtc_aurpc, mlp_aurpc, knc_aurpc, aggregate_aurpc],\n",
    "    'Precision': [abc_precision, etc_precision, rfc_precision, gbc_precision, xgb_precision, dtc_precision, mlp_precision, \n",
    "                  knc_precision, aggregate_precision],\n",
    "    'Recall' : [abc_recall, etc_recall, rfc_recall, gbc_recall, xgb_recall, dtc_recall, mlp_recall, knc_recall,\n",
    "                aggregate_recall]\n",
    "})\n",
    "# Print table and sort by test precision\n",
    "models = models.sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "blankIndex=[''] * len(models)\n",
    "models.index=blankIndex\n",
    "models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
