{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time as time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "# To Read out 1000 rows rather than the default value. \n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# To Read out 1000 columns rather than the default value. \n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "# Not prinitng the warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting a random seed for reproducability\n",
    "np.random.seed(7)\n",
    "\n",
    "# Setting up the k-fold\n",
    "kFold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)\n",
    "\n",
    "# Setting the working directory for data and output files\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd + '/data/')\n",
    "\n",
    "# Setting the environment for temporary results\n",
    "%env JOBLIB_TEMP_FOLDER = /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "(146294, 25)\n",
      "\n",
      "Cleaned Dataset Size:\n",
      "(145671, 15)\n",
      "\n",
      "Input Size: (145671, 13)\n",
      "Output Size: (145671,)\n"
     ]
    }
   ],
   "source": [
    "# Problem 1\n",
    "df = pd.read_csv('kplr_dr25_inj1_plti.csv', header = 0)\n",
    "\n",
    "print('Dataset Size:')\n",
    "print(df.shape)\n",
    "print()\n",
    "\n",
    "temp_df = df.iloc[:, 0:15]\n",
    "df_drop = temp_df[temp_df.isnull().any(axis=1)]\n",
    "temp_df = temp_df.drop(df_drop.index.values)\n",
    "temp_df = temp_df[temp_df.Recovered != 2]\n",
    "\n",
    "print('Cleaned Dataset Size:')\n",
    "print(temp_df.shape)\n",
    "print()\n",
    "\n",
    "X = temp_df.iloc[:, 1:14]\n",
    "Y = temp_df.iloc[:, 14]\n",
    "\n",
    "print('Input Size:', X.shape)\n",
    "print('Output Size:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Problem 2\\ndf = pd.read_csv('kplr_dr25_inj1_tces.csv', header = 0)\\n\\nprint('Dataset Size: ')\\nprint(df.shape)\\nprint()\\n\\ncols = ['TCE_ID', 'KIC', 'Disp', 'Score', 'period', 'epoch', 'NTL', 'SS', \\n        'CO', 'EM', 'Expected_MES', 'MES', 'NTran', 'depth', 'duration', 'Rp',\\n        'Rs', 'Ts', 'logg', 'a', 'Rp/Rs', 'a/Rs', 'impact', 'SNR_DV', 'Sp',\\n        'Fit_Prov']\\ndf = df[cols]\\ndf.columns\\n\\ndf['Disp'] = df['Disp'].replace('PC', 1)\\ndf['Disp'] = df['Disp'].replace('FP', 0)\\n\\nX = df.iloc[:, 10:25]\\nY = df.iloc[:, 2]\\n\\nprint('Input Size:', X.shape)\\nprint('Output Size:', Y.shape)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Problem 2\n",
    "df = pd.read_csv('kplr_dr25_inj1_tces.csv', header = 0)\n",
    "\n",
    "print('Dataset Size: ')\n",
    "print(df.shape)\n",
    "print()\n",
    "\n",
    "cols = ['TCE_ID', 'KIC', 'Disp', 'Score', 'period', 'epoch', 'NTL', 'SS', \n",
    "        'CO', 'EM', 'Expected_MES', 'MES', 'NTran', 'depth', 'duration', 'Rp',\n",
    "        'Rs', 'Ts', 'logg', 'a', 'Rp/Rs', 'a/Rs', 'impact', 'SNR_DV', 'Sp',\n",
    "        'Fit_Prov']\n",
    "df = df[cols]\n",
    "df.columns\n",
    "\n",
    "df['Disp'] = df['Disp'].replace('PC', 1)\n",
    "df['Disp'] = df['Disp'].replace('FP', 0)\n",
    "\n",
    "X = df.iloc[:, 10:25]\n",
    "Y = df.iloc[:, 2]\n",
    "\n",
    "print('Input Size:', X.shape)\n",
    "print('Output Size:', Y.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "gnb = GaussianNB()\n",
    "gpc = GaussianProcessClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "mlp = MLPClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  127.51\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "abc_predict = cross_val_predict(abc, X, Y, cv = kFold)\n",
    "abc_predict_proba = pd.DataFrame(cross_val_predict(abc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "abc_accuracy = metrics.accuracy_score(Y, abc_predict)  \n",
    "abc_precision = metrics.precision_score(Y, abc_predict, pos_label=1)\n",
    "abc_recall = metrics.recall_score(Y, abc_predict, pos_label=1)  \n",
    "abc_f1 = metrics.f1_score(Y, abc_predict, pos_label=1)\n",
    "abc_auroc = metrics.roc_auc_score(Y, abc_predict_proba[1])\n",
    "abc_aurpc = metrics.average_precision_score(Y, abc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  31.76\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dtc_predict = cross_val_predict(dtc, X, Y, cv = kFold)\n",
    "dtc_predict_proba = pd.DataFrame(cross_val_predict(dtc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "dtc_accuracy = metrics.accuracy_score(Y, dtc_predict)  \n",
    "dtc_precision = metrics.precision_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_recall = metrics.recall_score(Y, dtc_predict, pos_label=1)  \n",
    "dtc_f1 = metrics.f1_score(Y, dtc_predict, pos_label=1)\n",
    "dtc_auroc = metrics.roc_auc_score(Y, dtc_predict_proba[1])\n",
    "dtc_aurpc = metrics.average_precision_score(Y, dtc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  17.82\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "etc_predict = cross_val_predict(etc, X, Y, cv = kFold)\n",
    "etc_predict_proba = pd.DataFrame(cross_val_predict(etc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "etc_accuracy = metrics.accuracy_score(Y, etc_predict)  \n",
    "etc_precision = metrics.precision_score(Y, etc_predict, pos_label=1)\n",
    "etc_recall = metrics.recall_score(Y, etc_predict, pos_label=1)  \n",
    "etc_f1 = metrics.f1_score(Y, etc_predict, pos_label=1)\n",
    "etc_auroc = metrics.roc_auc_score(Y, etc_predict_proba[1])\n",
    "etc_aurpc = metrics.average_precision_score(Y, etc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  235.79\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gbc_predict = cross_val_predict(gbc, X, Y, cv = kFold)\n",
    "gbc_predict_proba = pd.DataFrame(cross_val_predict(gbc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "gbc_accuracy = metrics.accuracy_score(Y, gbc_predict)  \n",
    "gbc_precision = metrics.precision_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_recall = metrics.recall_score(Y, gbc_predict, pos_label=1)  \n",
    "gbc_f1 = metrics.f1_score(Y, gbc_predict, pos_label=1)\n",
    "gbc_auroc = metrics.roc_auc_score(Y, gbc_predict_proba[1])\n",
    "gbc_aurpc = metrics.average_precision_score(Y, gbc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.24\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gnb_predict = cross_val_predict(gnb, X, Y, cv = kFold)\n",
    "gnb_predict_proba = pd.DataFrame(cross_val_predict(gnb, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "gnb_accuracy = metrics.accuracy_score(Y, gnb_predict)  \n",
    "gnb_precision = metrics.precision_score(Y, gnb_predict, pos_label=1)\n",
    "gnb_recall = metrics.recall_score(Y, gnb_predict, pos_label=1)  \n",
    "gnb_f1 = metrics.f1_score(Y, gnb_predict, pos_label=1)\n",
    "gnb_auroc = metrics.roc_auc_score(Y, gnb_predict_proba[1])\n",
    "gnb_aurpc = metrics.average_precision_score(Y, gnb_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstart = time.time()\\ngpc_predict = cross_val_predict(gpc, X, Y, cv = kFold)\\ngpc_predict_proba = pd.DataFrame(cross_val_predict(gpc, X, Y, cv = kFold, method='predict_proba'))\\nend = time.time()\\n\\n# Store metrics\\ngpc_accuracy = metrics.accuracy_score(Y, gpc_predict)  \\ngpc_precision = metrics.precision_score(Y, gpc_predict, pos_label=1)\\ngpc_recall = metrics.recall_score(Y, gpc_predict, pos_label=1)  \\ngpc_f1 = metrics.f1_score(Y, gpc_predict, pos_label=1)\\ngpc_auroc = metrics.roc_auc_score(Y, gpc_predict_proba[1])\\ngpc_aurpc = metrics.average_precision_score(Y, gpc_predict, pos_label=1)\\n\\nprint('Time: ', round(end - start, 2))\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "gpc_predict = cross_val_predict(gpc, X, Y, cv = kFold)\n",
    "gpc_predict_proba = pd.DataFrame(cross_val_predict(gpc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "gpc_accuracy = metrics.accuracy_score(Y, gpc_predict)  \n",
    "gpc_precision = metrics.precision_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_recall = metrics.recall_score(Y, gpc_predict, pos_label=1)  \n",
    "gpc_f1 = metrics.f1_score(Y, gpc_predict, pos_label=1)\n",
    "gpc_auroc = metrics.roc_auc_score(Y, gpc_predict_proba[1])\n",
    "gpc_aurpc = metrics.average_precision_score(Y, gpc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  23.17\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "knc_predict = cross_val_predict(knc, X, Y, cv = kFold)\n",
    "knc_predict_proba = pd.DataFrame(cross_val_predict(knc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "knc_accuracy = metrics.accuracy_score(Y, knc_predict)  \n",
    "knc_precision = metrics.precision_score(Y, knc_predict, pos_label=1)\n",
    "knc_recall = metrics.recall_score(Y, knc_predict, pos_label=1)  \n",
    "knc_f1 = metrics.f1_score(Y, knc_predict, pos_label=1)\n",
    "knc_auroc = metrics.roc_auc_score(Y, knc_predict_proba[1])\n",
    "knc_aurpc = metrics.average_precision_score(Y, knc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  809.28\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mlp_predict = cross_val_predict(mlp, X, Y, cv = kFold)\n",
    "mlp_predict_proba = pd.DataFrame(cross_val_predict(mlp, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "mlp_accuracy = metrics.accuracy_score(Y, mlp_predict)  \n",
    "mlp_precision = metrics.precision_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_recall = metrics.recall_score(Y, mlp_predict, pos_label=1)  \n",
    "mlp_f1 = metrics.f1_score(Y, mlp_predict, pos_label=1)\n",
    "mlp_auroc = metrics.roc_auc_score(Y, mlp_predict_proba[1])\n",
    "mlp_aurpc = metrics.average_precision_score(Y, mlp_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  49.82\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rfc_predict = cross_val_predict(rfc, X, Y, cv = kFold)\n",
    "rfc_predict_proba = pd.DataFrame(cross_val_predict(rfc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "rfc_accuracy = metrics.accuracy_score(Y, rfc_predict)  \n",
    "rfc_precision = metrics.precision_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_recall = metrics.recall_score(Y, rfc_predict, pos_label=1)  \n",
    "rfc_f1 = metrics.f1_score(Y, rfc_predict, pos_label=1)\n",
    "rfc_auroc = metrics.roc_auc_score(Y, rfc_predict_proba[1])\n",
    "rfc_aurpc = metrics.average_precision_score(Y, rfc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstart = time.time()\\nsvc_predict = cross_val_predict(svc, X, Y, cv = kFold)\\nsvc_predict_proba = pd.DataFrame(cross_val_predict(svc, X, Y, cv = kFold, method='predict_proba'))\\nend = time.time()\\n\\n# Store metrics\\nsvc_accuracy = metrics.accuracy_score(Y, svc_predict)  \\nsvc_precision = metrics.precision_score(Y, svc_predict, pos_label=1)\\nsvc_recall = metrics.recall_score(Y, svc_predict, pos_label=1)  \\nsvc_f1 = metrics.f1_score(Y, svc_predict, pos_label=1)\\nsvc_auroc = metrics.roc_auc_score(Y, svc_predict_proba[1])\\nsvc_aurpc = metrics.average_precision_score(Y, svc_predict, pos_label=1)\\n\\nprint('Time: ', round(end - start, 2))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "svc_predict = cross_val_predict(svc, X, Y, cv = kFold)\n",
    "svc_predict_proba = pd.DataFrame(cross_val_predict(svc, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "svc_accuracy = metrics.accuracy_score(Y, svc_predict)  \n",
    "svc_precision = metrics.precision_score(Y, svc_predict, pos_label=1)\n",
    "svc_recall = metrics.recall_score(Y, svc_predict, pos_label=1)  \n",
    "svc_f1 = metrics.f1_score(Y, svc_predict, pos_label=1)\n",
    "svc_auroc = metrics.roc_auc_score(Y, svc_predict_proba[1])\n",
    "svc_aurpc = metrics.average_precision_score(Y, svc_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  166.35\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "xgb_predict = cross_val_predict(xgb, X, Y, cv = kFold)\n",
    "xgb_predict_proba = pd.DataFrame(cross_val_predict(xgb, X, Y, cv = kFold, method='predict_proba'))\n",
    "end = time.time()\n",
    "\n",
    "# Store metrics\n",
    "xgb_accuracy = metrics.accuracy_score(Y, xgb_predict)  \n",
    "xgb_precision = metrics.precision_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_recall = metrics.recall_score(Y, xgb_predict, pos_label=1)  \n",
    "xgb_f1 = metrics.f1_score(Y, xgb_predict, pos_label=1)\n",
    "xgb_auroc = metrics.roc_auc_score(Y, xgb_predict_proba[1])\n",
    "xgb_aurpc = metrics.average_precision_score(Y, xgb_predict, pos_label=1)\n",
    "\n",
    "print('Time: ', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'ADABoost': abc_predict,\n",
    "    'ExtraTrees': etc_predict,\n",
    "    'RandomForest': rfc_predict,\n",
    "    'GradientBoosting': gbc_predict,\n",
    "    'XGBoost': xgb_predict,\n",
    "    'DecisionTree': dtc_predict,\n",
    "    'MultiLayerPerceptron': mlp_predict,\n",
    "    'KNeighbors': knc_predict,\n",
    "    'NaiveBayes': gnb_predict\n",
    "} \n",
    "\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metrics\n",
    "predictions['Aggregate'] = predictions.mean(axis = 1)\n",
    "aggregate_auroc = metrics.roc_auc_score(Y, predictions['Aggregate'])\n",
    "\n",
    "predictions['Aggregate'] = round(predictions['Aggregate']).astype(int)\n",
    "aggregate_accuracy = metrics.accuracy_score(Y, predictions['Aggregate'])  \n",
    "aggregate_precision = metrics.precision_score(Y, predictions['Aggregate'], pos_label = 1)\n",
    "aggregate_recall = metrics.recall_score(Y, predictions['Aggregate'], pos_label = 1)  \n",
    "aggregate_f1 = metrics.f1_score(Y, predictions['Aggregate'], pos_label = 1)\n",
    "aggregate_aurpc = metrics.average_precision_score(Y, predictions['Aggregate'], pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AURPC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.889450</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.956928</td>\n",
       "      <td>0.727988</td>\n",
       "      <td>0.815915</td>\n",
       "      <td>0.827063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.889312</td>\n",
       "      <td>0.821752</td>\n",
       "      <td>0.956764</td>\n",
       "      <td>0.727676</td>\n",
       "      <td>0.813870</td>\n",
       "      <td>0.829787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Aggregate</td>\n",
       "      <td>0.889223</td>\n",
       "      <td>0.820885</td>\n",
       "      <td>0.924018</td>\n",
       "      <td>0.727509</td>\n",
       "      <td>0.816255</td>\n",
       "      <td>0.825568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>ADA Boost</td>\n",
       "      <td>0.887013</td>\n",
       "      <td>0.819431</td>\n",
       "      <td>0.953368</td>\n",
       "      <td>0.722775</td>\n",
       "      <td>0.805565</td>\n",
       "      <td>0.833784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.881033</td>\n",
       "      <td>0.799774</td>\n",
       "      <td>0.942286</td>\n",
       "      <td>0.710306</td>\n",
       "      <td>0.828788</td>\n",
       "      <td>0.772722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.868423</td>\n",
       "      <td>0.771552</td>\n",
       "      <td>0.931884</td>\n",
       "      <td>0.683323</td>\n",
       "      <td>0.827589</td>\n",
       "      <td>0.722623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.845707</td>\n",
       "      <td>0.749599</td>\n",
       "      <td>0.820379</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>0.748115</td>\n",
       "      <td>0.751088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Multi Layer Perceptron</td>\n",
       "      <td>0.838520</td>\n",
       "      <td>0.747333</td>\n",
       "      <td>0.930184</td>\n",
       "      <td>0.627974</td>\n",
       "      <td>0.720129</td>\n",
       "      <td>0.776674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>K Neighbors</td>\n",
       "      <td>0.717404</td>\n",
       "      <td>0.487399</td>\n",
       "      <td>0.719908</td>\n",
       "      <td>0.413898</td>\n",
       "      <td>0.551032</td>\n",
       "      <td>0.436940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.455932</td>\n",
       "      <td>0.521889</td>\n",
       "      <td>0.860136</td>\n",
       "      <td>0.355842</td>\n",
       "      <td>0.357559</td>\n",
       "      <td>0.965730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy        F1     AUROC     AURPC  Precision  \\\n",
       "       Gradient Boosting  0.889450  0.821452  0.956928  0.727988   0.815915   \n",
       "                XG Boost  0.889312  0.821752  0.956764  0.727676   0.813870   \n",
       "               Aggregate  0.889223  0.820885  0.924018  0.727509   0.816255   \n",
       "               ADA Boost  0.887013  0.819431  0.953368  0.722775   0.805565   \n",
       "           Random Forest  0.881033  0.799774  0.942286  0.710306   0.828788   \n",
       "             Extra Trees  0.868423  0.771552  0.931884  0.683323   0.827589   \n",
       "           Decision Tree  0.845707  0.749599  0.820379  0.638436   0.748115   \n",
       "  Multi Layer Perceptron  0.838520  0.747333  0.930184  0.627974   0.720129   \n",
       "             K Neighbors  0.717404  0.487399  0.719908  0.413898   0.551032   \n",
       "             Naive Bayes  0.455932  0.521889  0.860136  0.355842   0.357559   \n",
       "\n",
       "    Recall  \n",
       "  0.827063  \n",
       "  0.829787  \n",
       "  0.825568  \n",
       "  0.833784  \n",
       "  0.772722  \n",
       "  0.722623  \n",
       "  0.751088  \n",
       "  0.776674  \n",
       "  0.436940  \n",
       "  0.965730  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model comparison\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['ADA Boost', 'Extra Trees', 'Random Forest', 'Gradient Boosting', 'XG Boost', 'Decision Tree',\n",
    "              'Multi Layer Perceptron', 'K Neighbors', 'Naive Bayes', 'Aggregate'],\n",
    "    'Accuracy' : [abc_accuracy, etc_accuracy, rfc_accuracy, gbc_accuracy, xgb_accuracy, dtc_accuracy,\n",
    "                  mlp_accuracy, knc_accuracy, gnb_accuracy, aggregate_accuracy],\n",
    "    'F1' : [abc_f1, etc_f1, rfc_f1, gbc_f1, xgb_f1, dtc_f1, mlp_f1, knc_f1, gnb_f1, aggregate_f1],\n",
    "    'AUROC' : [abc_auroc, etc_auroc, rfc_auroc, gbc_auroc, xgb_auroc, dtc_auroc, mlp_auroc, knc_auroc, \n",
    "               gnb_auroc, aggregate_auroc],\n",
    "    'AURPC' : [abc_aurpc, etc_aurpc, rfc_aurpc, gbc_aurpc, xgb_aurpc, dtc_aurpc, mlp_aurpc, knc_aurpc,\n",
    "               gnb_aurpc, aggregate_aurpc],\n",
    "    'Precision': [abc_precision, etc_precision, rfc_precision, gbc_precision, xgb_precision, dtc_precision, mlp_precision, \n",
    "                  knc_precision, gnb_precision, aggregate_precision],\n",
    "    'Recall' : [abc_recall, etc_recall, rfc_recall, gbc_recall, xgb_recall, dtc_recall, mlp_recall, knc_recall,\n",
    "                gnb_recall, aggregate_recall]\n",
    "})\n",
    "# Print table and sort by test precision\n",
    "models = models.sort_values(by = 'Accuracy', ascending = False)\n",
    "\n",
    "blankIndex = [''] * len(models)\n",
    "models.index = blankIndex\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.to_csv('baseline.csv', sep = ',', encoding = 'utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
